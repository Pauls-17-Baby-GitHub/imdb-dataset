{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wwnnwv8wQ0N-h6iyeK8MFeKzijJTeg7q",
      "authorship_tag": "ABX9TyMe01h5zAHFxu9NfrgYYKcV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pauls-Baby/imdb-dataset/blob/master/imdb_plot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dsMt2QIiW0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets\n",
        "import random\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvWFZysKidHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class load_data(object):\n",
        "    def __init__(self, SEED=1234):\n",
        "        torch.manual_seed(SEED)\n",
        "        torch.cuda.manual_seed(SEED)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "        TEXT = data.Field(tokenize='spacy')\n",
        "        LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "        self.train_data, self.test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "        self.SEED = SEED\n",
        "\n",
        "    def get_fold_data(self, num_folds=3):\n",
        "        TEXT = data.Field(tokenize='spacy')\n",
        "        LABEL = data.LabelField(dtype=torch.float)\n",
        "        fields = [('text', TEXT), ('label', LABEL)]\n",
        "        \n",
        "        kf = KFold(n_splits=num_folds, shuffle=True, random_state=self.SEED)\n",
        "        train_data_arr = np.array(self.train_data.examples)\n",
        "        print(\"[GENERATING 3-FOLD DATA]\")\n",
        "        for train_index, val_index in kf.split(train_data_arr):\n",
        "            yield(\n",
        "                TEXT,\n",
        "                LABEL,\n",
        "                data.Dataset(train_data_arr[train_index], fields=fields),\n",
        "                data.Dataset(train_data_arr[val_index], fields=fields),\n",
        "            )\n",
        "\n",
        "    def get_test_data(self):\n",
        "        return self.test_data"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9zFfuAOjNTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Self_Attention(nn.Module):\n",
        "    def __init__(self, query_dim):\n",
        "        # assume: query_dim = key/value_dim\n",
        "        super(Self_Attention, self).__init__()\n",
        "        self.scale = 1. / math.sqrt(query_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        # query == hidden: (batch_size, hidden_dim * 2)\n",
        "        # key/value == gru_output: (sentence_length, batch_size, hidden_dim * 2)\n",
        "        query = query.unsqueeze(1) # (batch_size, 1, hidden_dim * 2)\n",
        "        key = key.transpose(0, 1).transpose(1, 2) # (batch_size, hidden_dim * 2, sentence_length)\n",
        "\n",
        "        # bmm: batch matrix-matrix multiplication\n",
        "        attention_weight = torch.bmm(query, key) # (batch_size, 1, sentence_length)\n",
        "        attention_weight = F.softmax(attention_weight.mul_(self.scale), dim=2) # normalize sentence_length's dimension\n",
        "\n",
        "        value = value.transpose(0, 1) # (batch_size, sentence_length, hidden_dim * 2)\n",
        "        attention_output = torch.bmm(attention_weight, value) # (batch_size, 1, hidden_dim * 2)\n",
        "        attention_output = attention_output.squeeze(1) # (batch_size, hidden_dim * 2)\n",
        "\n",
        "        return attention_output, attention_weight.squeeze(1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSscPmINjRgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, d_rate,\n",
        "                embedding_weights=None, embedding_trainable=True):\n",
        "        super(Model, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if embedding_weights is not None:\n",
        "            self.embedding.weight.data.copy_(embedding_weights)\n",
        "        if embedding_trainable is False:\n",
        "            self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, \n",
        "                        bidirectional=True, dropout=d_rate)\n",
        "        self.dense = nn.Linear(2 * hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(d_rate)\n",
        "        self.attention = Self_Attention(2 * hidden_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: (sentence_length, batch_size)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        # embedded: (sentence_length, batch_size, embedding_dim)\n",
        "\n",
        "        gru_output, hidden = self.gru(embedded)\n",
        "        # gru_output: (sentence_length, batch_size, hidden_dim * 2)\n",
        "        ## depth_wise\n",
        "        # hidden: (num_layers * 2, batch_size, hidden_dim)\n",
        "        ## ordered: [f_layer_0, b_layer_0, ...f_layer_n, b_layer n]\n",
        "\n",
        "        # concat the final output of forward direction and backward direction\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        # hidden: (batch_size, hidden_dim * 2)\n",
        "\n",
        "        rescaled_hidden, attention_weight = self.attention(query=hidden, key=gru_output, value=gru_output)\n",
        "        output = self.dense(rescaled_hidden)\n",
        "\n",
        "        return output.squeeze(1), attention_weight"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyztKO_7jeGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEbNNPUbkznF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cdaad725-4c06-4509-cdfc-79e498c16905"
      },
      "source": [
        "args_p = Path(\"/content/drive/My Drive/imdb/param.json\")\n",
        "if args_p.exists() is False:\n",
        "    raise Exception('Path not found. Please check path to paramters json file!')\n",
        "else:\n",
        "    print(\"Hyperparameters are chosen from JSON file\")\n",
        "\n",
        "with args_p.open(mode='r') as f:\n",
        "    true = True\n",
        "    false = False\n",
        "    null = None\n",
        "    args = json.load(f)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters are chosen from JSON file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ-tu9eYmFWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_run(model, iterator, optimizer, criterion):\n",
        "    train_accuracy = []\n",
        "    train_loss = []\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.train()\n",
        "    print(\"TRAINING:\",end=\" \")\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(batch.text)\n",
        "        loss = criterion(output, batch.label)\n",
        "        acc = binary_accuracy(output, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        print(\"/\",end=\" \")\n",
        "    train_accuracy.append(epoch_acc)\n",
        "    train_loss.append(epoch_loss)\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), train_accuracy, train_loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJZfG_AkmN5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_run(model, iterator, criterion):\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    print(\"VALIDATION:\",end=\" \")\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions, _ = model(batch.text)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            print(\"/\",end=\" \")\n",
        "    val_accuracy.append(epoch_acc)\n",
        "    val_loss.append(epoch_loss)\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), val_accuracy, val_loss"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGsZN4x4qvhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6snkw0Dqn_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logfile = str('/content/drive/My Drive/log/log-{}.txt'.format(run_start_time))\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[\n",
        "                        logging.FileHandler(logfile),\n",
        "                        logging.StreamHandler(sys.stdout)\n",
        "                    ])\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxlF_hgJnQUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    data_generator = load_data()\n",
        "    _history = []\n",
        "    device = None\n",
        "    model = None\n",
        "    criterion = None\n",
        "    fold_index = 0\n",
        "    print(\"[DATA LOADED]\")\n",
        "    for TEXT, LABEL, train_data, val_data in data_generator.get_fold_data(num_folds=args['num_folds']):\n",
        "        logger.info(\"[RUNNING TRAINING]\")\n",
        "        logger.info(f\"Now fold: {fold_index + 1} / {args['num_folds']}\")\n",
        "\n",
        "        TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.300d\")\n",
        "        logger.info(f'Embedding size: {TEXT.vocab.vectors.size()}.')\n",
        "        LABEL.build_vocab(train_data) # For converting str into float labels.\n",
        "        print(\"__________________________\")\n",
        "        model = Model(len(TEXT.vocab), args['embedding_dim'], args['hidden_dim'],\n",
        "            args['output_dim'], args['num_layers'], args['dropout'], TEXT.vocab.vectors, args[\"embedding_trainable\"])\n",
        "        print(\"[MODEL GENERATED]\")       \n",
        "        optimizer = optim.Adam(model.parameters())\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        if args['gpu'] is True and args['gpu_number'] is not None:\n",
        "            torch.cuda.set_device(args['gpu_number'])\n",
        "            device = torch.device('cuda')\n",
        "            model = model.to(device)\n",
        "            criterion = criterion.to(device)\n",
        "            print(\"\\n[GPU USED FOR EXECUTION]\")\n",
        "        else:\n",
        "            device = torch.device('cpu')\n",
        "            model = model.to(device)\n",
        "            criterion = criterion.to(device)\n",
        "            print(\"\\n[CPU USED FOR EXECUTION]\")\n",
        "        \n",
        "        train_iterator = data.Iterator(train_data, batch_size=args['batch_size'], sort_key=lambda x: len(x.text), device=device)\n",
        "        val_iterator = data.Iterator(val_data, batch_size=args['batch_size'], sort_key=lambda x: len(x.text), device=device)\n",
        "        print(\"[ITERATORS GENERATED]\")\n",
        "        for epoch in range(args['epochs']):\n",
        "            train_loss, train_acc, train_accuracy_list, train_loss_list = train_run(model, train_iterator, optimizer, criterion)\n",
        "            print(\"\\n\")\n",
        "            logger.info(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        val_loss, val_acc, val_accuracy_list, val_loss_list = eval_run(model, val_iterator, criterion)\n",
        "        print(\"\\n\")\n",
        "        logger.info(f'Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc*100:.2f}% |')\n",
        "\n",
        "        _history.append([val_loss, val_acc])\n",
        "        fold_index += 1\n",
        "        print(\"[FOLD OVER]\\n\")\n",
        "        print(\"#######################################\")\n",
        "    _history = np.asarray(_history)\n",
        "    loss = np.mean(_history[:, 0])\n",
        "    acc = np.mean(_history[:, 1])\n",
        "    \n",
        "    logger.info('[CROSS VALIDATION RESULTS]')\n",
        "    logger.info(f'LOSS: {loss}, ACC: {acc}')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hHOJy7Qn6WX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "outputId": "031eaeff-c6c0-4973-a54b-16745dc2677f"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[DATA LOADED]\n",
            "[GENERATING 3-FOLD DATA]\n",
            "08/01/2020 12:37:46 - INFO - __main__ -   [RUNNING TRAINING]\n",
            "08/01/2020 12:37:46 - INFO - __main__ -   Now fold: 1 / 2\n",
            "08/01/2020 12:37:47 - INFO - torchtext.vocab -   Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "08/01/2020 12:37:48 - INFO - __main__ -   Embedding size: torch.Size([25002, 300]).\n",
            "__________________________\n",
            "[MODEL GENERATED]\n",
            "\n",
            "[GPU USED FOR EXECUTION]\n",
            "[ITERATORS GENERATED]\n",
            "TRAINING: / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / \n",
            "\n",
            "08/01/2020 12:43:54 - INFO - __main__ -   | Epoch: 01 | Train Loss: 0.432 | Train Acc: 78.17%\n",
            "TRAINING: / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / \n",
            "\n",
            "08/01/2020 12:50:00 - INFO - __main__ -   | Epoch: 02 | Train Loss: 0.215 | Train Acc: 91.68%\n",
            "VALIDATION: / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / \n",
            "\n",
            "08/01/2020 12:51:59 - INFO - __main__ -   Val. Loss: 0.251 | Val. Acc: 90.02% |\n",
            "[FOLD OVER]\n",
            "\n",
            "#######################################\n",
            "08/01/2020 12:51:59 - INFO - __main__ -   [RUNNING TRAINING]\n",
            "08/01/2020 12:51:59 - INFO - __main__ -   Now fold: 2 / 2\n",
            "08/01/2020 12:52:00 - INFO - torchtext.vocab -   Loading vectors from .vector_cache/glove.6B.300d.txt.pt\n",
            "08/01/2020 12:52:01 - INFO - __main__ -   Embedding size: torch.Size([25002, 300]).\n",
            "__________________________\n",
            "[MODEL GENERATED]\n",
            "\n",
            "[GPU USED FOR EXECUTION]\n",
            "[ITERATORS GENERATED]\n",
            "TRAINING: / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / \n",
            "\n",
            "08/01/2020 12:58:18 - INFO - __main__ -   | Epoch: 01 | Train Loss: 0.473 | Train Acc: 75.22%\n",
            "TRAINING: / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / \n",
            "\n",
            "08/01/2020 13:04:37 - INFO - __main__ -   | Epoch: 02 | Train Loss: 0.219 | Train Acc: 91.48%\n",
            "VALIDATION: / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / \n",
            "\n",
            "08/01/2020 13:06:32 - INFO - __main__ -   Val. Loss: 0.236 | Val. Acc: 90.81% |\n",
            "[FOLD OVER]\n",
            "\n",
            "#######################################\n",
            "08/01/2020 13:06:32 - INFO - __main__ -   [CROSS VALIDATION RESULTS]\n",
            "08/01/2020 13:06:32 - INFO - __main__ -   LOSS: 0.24344774696242322, ACC: 0.9041640025270565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP_ofrqS-5TR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.arange(0,2,0.1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxq_tMfX0Qob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "519bc91a-6fdb-4973-d906-3ac9fa798c22"
      },
      "source": [
        "# Plot for variation in scales\n",
        "plt.plot(x_train, train_accuracy_list)\n",
        "plt.xlabel('Accuracy', fontsize=16)\n",
        "plt.ylabel('Epoch', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Training Accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-58bcba0dd95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot for variation in scales\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_accuracy_list' is not defined"
          ]
        }
      ]
    }
  ]
}